import os
import logging
from urllib.parse import urlparse
from scrapy.crawler import CrawlerProcess
from scraper.spider import WebSpider
from processing.analyzer import ContentAnalyzer
from interaction.cli import start_interaction

def configurar_logs(verbose=False):
    """Configura los niveles de logging"""
    nivel = logging.DEBUG if verbose else logging.ERROR
    logging.basicConfig(level=nivel)
    logging.getLogger('scrapy').setLevel(nivel)
    logging.getLogger('httpx').setLevel(nivel)
    logging.getLogger('httpcore').setLevel(nivel)

def main():
    # Configuraci√≥n inicial
    verbose = input("¬øMostrar logs detallados? (s/n): ").lower() == 's'
    configurar_logs(verbose)

    url = input("üåê Ingresa la URL a analizar: ")
    hostname = urlparse(url).hostname
    raw_dir = os.path.abspath(f"data/raw/{hostname}/pages")
    processed_dir = os.path.abspath(f"data/processed/{hostname}")

    # Crear directorios
    os.makedirs(raw_dir, exist_ok=True)
    os.makedirs(processed_dir, exist_ok=True)

    # Manejo de scraping
    existing_files = [f for f in os.listdir(raw_dir) if f.endswith(".json")]
    
    if existing_files:
        print(f"üìÇ Se encontraron {len(existing_files)} archivos scrapeados.")
        rescrapear = input("¬øDeseas re-scrapear? (s/n): ").lower() == 's'
        if rescrapear:
            print("\nüï∑Ô∏è Iniciando scraping...")
            process = CrawlerProcess(settings={'LOG_LEVEL': 'ERROR' if not verbose else 'DEBUG'})
            process.crawl(WebSpider, url=url)
            process.start()
            existing_files = [f for f in os.listdir(raw_dir) if f.endswith(".json")]

    # Opci√≥n de procesamiento
    procesar = True
    if existing_files and not rescrapear:
        procesar = input("¬øDeseas procesar los datos existentes? (s/n): ").lower() == 's'

    # Procesamiento de datos
    processed_files = []
    if procesar:
        analyzer = ContentAnalyzer()
        for file in existing_files:
            full_path = os.path.join(raw_dir, file)
            try:
                processed_file = analyzer.analyze(full_path)
                processed_files.append(processed_file)
                print(f"‚úÖ Procesado: {processed_file}")
            except Exception as e:
                print(f"‚ùå Error procesando {file}: {str(e)}")

    # Interacci√≥n
    if processed_files:
        start_interaction(processed_files[0], url)
    else:
        print("\n‚ö†Ô∏è No hay datos disponibles para interactuar")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nüõë Ejecuci√≥n interrumpida por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error cr√≠tico: {str(e)}")